[{"content":"前言： HarmonyOS NEXT的生态建设真是一个巨坑，特别是对于 独立开发者与开源项目 来说，更不用说元服务面对普通应用天然劣势，简直是来自审核和竞品的双重压力。但是怀着不能让新兴平台被粗制滥造的应用所淹没的信念，我还是坚持把这个项目的主要功能完成了，版本号从 v0.1.0 数到了 v1.0.0。 🎉🎉🎉 完结撒花 🎉🎉🎉\nP.S. 截至本文章发布时，Aigis Authenticator v1.0.0 仍在审核中，虽然早于12月27日上午提交审核，但大概要被拖到2025年了 😅\n项目简介 Aigis 是一个服务于 HarmonyOS NEXT 的 2FA 认证元服务，采用纯 ArkTS 实现，无任何三方依赖。\nAigis 的目标是成为安卓平台 Aegis Authenticator 的轻量级替代品。当前，凭借不到 300KB 的安装包大小，Aigis 已实现了其超过 90% 的功能，且实现了存储数据格式的完整兼容，可以支持加密备份文件的相互导入导出。同时，Aigis 也在设计上做了一些改进，提供了更好的用户体验和数据安全性。\n开发动机 我在 2024年9月 升级到 HarmonyOS NEXT beta 版本后，被迫放弃了大量安卓软件，以及忍受各种有缺陷的应用功能，但还是发现一个问题不能解决：没有 Aegis Authenticator 的替代品。\n当时的应用市场中，只有一个叫 “手机令牌” 的丑到爆的软件（目前竟然还在更新！还是那么丑！），还不存在同类竞品虽然之后卓易通的上架证明大家都是小丑🤡。同时，我本来计划开发一个 RetroArch 的鸿蒙前端，Retrohos，正好积累一下 GUI 的开发经验。于是我在国庆假期期间，快速学习了下 HarmonyOS NEXT 的开发文档，并在 假期结束前 完成了这个项目的核心功能：OTP 计算与展示功能。在于 10月9日 提交备案审核，10月15日 过审后，我于当日上架了 Aigis 的首个测试版本。\n设计理念 Aigis 的主体设计语言参考了原版 Aegis Authenticator，同时因为元服务的形态做出了一定调整；主题配色大致参考了Aigis。\n主页列表条目参考了当前系统应用 邮件 的设计，编辑页面采用了 Aegis Authenticator 的布局方案。设置页项条目参考了系统 设置 的设计，优先使用了系统提供的组件范式，以保证用户体验的一致性。\nAigis Authenticator 在设计中始终把 安全 放在第一位。\n虽然相较于普通鸿蒙应用，元服务无法使用 Asset Kit 数据存储，系统备份，云同步等API，但这也从原理上保证了元服务的安全性：在没有网络权限的前提下，除非得到用户的许可，用户的数据无法从应用沙箱中逃逸。\n相对的，在数据存储方面，Aigis 参考 Aegis Authenticator 的设计，将条目密钥存储在文件中（用户首选项）。同时，用户需要设置主密钥，并经过经典 PBKDF2 派生算法，生成 AES-256 密钥并保存在可信执行环境（TEE）中，用于条目密钥的加解密使用。\n在这一数据安全存储的机制下，Aigis 通过以下的一系列安全设计，保证用户对数据的绝对掌控：\n条目/密钥分离存储：在用户首选项文件中，条目和密钥分别存储，只对密钥进行加密，实现了 页面渲染效率（仅需条目）和 令牌计算（需要条目+密钥）的安全性 的平衡。在导出备份文件时，条目和密钥分别使用独立的密钥材料加密，进一步提升了数据的安全性。\n加密/解密密钥分离设计：得益于 HUKS 的设计，Aigis 将派生的主密钥存储在可信执行环境（TEE）中，并根据用途分为两个密钥：一个用于加密密钥（enc_master_key），不需要鉴权即可调用；另一个用于解密密钥（dec_master_key），严格限制在应用中调用，在 v2.0.0 之后支持在 ATL3 级别安全验证之后才可以调用。\n加密JSON文件备份：用户在设置主密钥后，可以导出加密的 JSON 格式数据文件，用于备份或者迁移到另一 Aigis 实例中。同时，用户可在 PC 或其他设备上，通过输入主密钥并调用 PBKDF2 算法，自由解密数据文件。\n生物识别解锁：Aigis 支持 ATL1 级别的生物识别解锁应用界面，同时允许设置 面部/指纹/PIN码 用于解锁的优先级。与 Aegis Authenticator 不同的是，Aigis 在未设置主密钥时也支持启用生物解锁，该情况下您的密钥仍以明文存储在应用沙箱中。从 v2.0.0 开始，Aigis 将支持 ATL3 级别的生物解锁，该安全性将与 Aegis Authenticator 生物解锁安全性等价。\n隐私窗口保护：Aigis 默认不允许截屏、屏幕录制、任意拖拽内容，以避免可能的个人信息泄露；另一方面，从 v0.8.0 版本开始，用户在设置主密钥后，可以设置禁用隐私窗口保护，以便在需要时截屏。（需要注意的是，在设置主密钥后，条目的二维码分享功能将会被自动禁用）\n特性一览 主页面列表，支持条目拖拽排序，搜索，置顶\n编辑页面支持 二维码分享 条目\n仅当主密钥未设置时，设置主密钥后，分享功能将被自动禁用 通用/外观设置，对齐 Aegis Authenticator 界面定制需求\n支持 Aegis 格式图标包导入\n主密钥设置，密码周期提醒，屏幕安全，生物识别解锁\n扫码支持 Google Authenticator 二维码导入导出\n纯 ArkTS 实现 otpauth-migration payload Protobuf 编解码 支持 Aegis Authenticator 加密备份文件导入导出\n开发避坑指南 以下内容仅涉及本人在开发 Aigis 过程中摸索遇到的问题，并不全面，仅供参考。\n开发文档的获取 虽然论坛里很多吐槽开发文档不好查看，查找一个API的用法要跳转好几个页面（事实也确实如此），但是这个开发文档是和 OpenHarmony文档 保持一致的，所以在查看文档时需要分清主次。\n首先，指南 和 API参考，这两个和 OpenHarmony 保持一致，随版本号发布（虽然 OpenHarmony 网站只放出了 5.0.0 的文档，但 5.0.1 (API 13) 的文档在 gitee仓库 确实是存在的）。\n其次，最佳实践 和 FAQ，这两个是 HarmonyOS NEXT 特有的文档，更新频率会高一些（也没高多少），主要是对开发文档的缺失部分做快速的补充，但具体功能的缺陷解决和特性，目前来看还是要等上游 OpenHarmony 发版之后才能同步。另外，变更预告 也有一定用处，但我是看一次失望一次，反正是没修复过我提到的问题。\n除此之外，在线文档页面的“智能客服”以及全局搜索功能，也有一定用处，但是不要抱太大希望，毕竟现在生态还在建设中，很多问题都是需要自己摸索。\n备案与软件著作权 关于备案：满足两个条件，单机 + 元服务，就不需要备案了；但是需要内置应用启动时弹出的“用户协议”和“隐私协议”，具体内容请参考现有应用的协议，也可以搜索论坛里的帖子，有人分享过。如果是其他情况，则一定需要备案，建议在应用启动开发时，就开始启动这个流程；这样在应用基本功能完成时，备案也差不多完成审核了。\n关于软件著作权：元服务不需要软件著作权，应用需要软件著作权，但也可以用电子版权证书代替；不想让代理公司赚钱，可以自己申请，但是时间要长一些。\n工单系统的重要性 在开发过程中，遇到疑似 API缺陷/文档缺陷 的问题，首先请在在开发者联盟论坛里搜索一下, 看看有没有人遇到过类似的问题。\n如果没有的话，建议也不要再发帖了，而是直接在开发者联盟的工单系统里提交工单，这样可以更快的得到官方的回复。\n虽然工单系统的回复也是千篇一律的客服复制粘贴文档，但如果问题确实存在的话，还是会上升到开发团队的视野里，有可能会得到解决。\n而如果是关于应用上架遇到问题，则需要在 AppGalley Connect 的“互动中心”提交工单，这样可以更快的得到审核人员的回复。\nHUKS 密钥管理与使用 HUKS 的文档是存在各种缺陷的，它基于设备提供的 TEE 来实现，能力边界是和设备高度耦合的，但文档上却没体现这一点。直接影响就是，有些API在手机端根本没法用，但他们的测试却没有问题。\n这里列举下我个人遇到的问题，论坛上的相关讨论是比较少的：\n测试可用的：密钥导入，密钥删除，密钥覆盖创建，AES 相关加解密操作，auth鉴权访问，我这边试过都是没问题的。如有需要的话，可以参考我封装自用的工具类 HUKSUtils。\nHMAC 方法最好不要用，具体来说，也就是不要期待可以在 TEE 里完成基于 SHA1/SHA256 的，非标准密钥长度（256bit 之外）的 HMAC 操作；如果你不信的话，可以写个测试用例试试，如果能用的话也可以告诉我一声 ：）\n密钥派生方法：强烈建议不要使用！ 首先，它只有 PBKDF2 派生算法，还不是标准实现！它的派生密钥是不能在其他设备上重复生成的，而是和TEE里的AES密钥及不可控的加密参数相关！鸿蒙使用了私有的派生算法，这个算法的安全性和可靠性都是未知的，不建议使用！\n考虑到当前 ArkTS 虚拟机对于加密算法的效率问题，推荐使用 openssl 绑定的加密库，如 @ohos-rs/openssl，或者我手搓了一个 Scrypt 密钥派生算法（仅8KB大小）。\nArkTS 虚拟机效率问题 正如上面提到的，ArkTS 虚拟机对于加密算法的效率问题，是一个很大的问题。早在API 10的版本，就有人反馈过 @ohos/crypto-js 加解密卡死 的问题，但直到目前为止，官方还没有给出解决方案。\n我曾经在论坛提出这个问题，ArkTS 计算密集任务（XOR运算）效率低下，同时也在工单系统提问，但最后的结论是，必须把相关代码切换到 Native 实现，AOT当前是无法启用的（更不用说JIT）。\n另一方面，华为也在推广 Cangjie 作为 ArkTS 的接班人，但从我的角度来看，Cangjie 的审美比 TypeScript 或者 Rust 都差远了，而且它的文档也是一团糟，API 也还没有和 ArkTS 保持一致，谁爱用谁用吧 ╮(╯▽╰)╭\n元服务 Native 开发与 Native 方法调用 元服务是不允许创建 Native Module 的，也就是说不能在 元服务项目 里创建/编译含 Native 代码的 HAR/HSP module。但是，元服务可以通过引用 HAR/HSP 模块，调用 Native 方法 …… 很明显所谓不能创建 Native Module只是 DevEco Studio 的缺陷，元服务的地位之低可见一斑。\n在元服务里调用 Native 方法，需要注意：对于含有 Native 模块的 HAR/HSP 模块，元服务不能通过 在线 方式引用，而需要把对应的库放在本地，然后通过本地引用的方式来调用，不然会无法找到 NAPI 暴露的方法，报错如：the requested module '@normalized:Y\u0026amp;\u0026amp;\u0026amp;libscrypt.so\u0026amp;' does not provide an export name 'parallelROMix' which imported by '\u0026amp;scrypt/src/main/ets/scrypt\u0026amp;2.0.0'。 而神奇的是，对于普通应用，则完全没有这个问题。\n这个问题我也提过工单了，就是到现在还没人理 😅 心路历程 这个吐槽部分放在最后，只是来看技术的读者可以直接跳过。\n2024年12月31日前，HarmonyOS NEXT的生态进展只有一个主旋律：冲量。\n企业开发者方面： 先是威逼利诱头部厂商，做出99%的日常应用；然后上架 卓易通/出境易 安卓虚拟机，补齐剩下的小众软件境外软件。\n个人开发者方面： 开发者激励计划，放出大量红利，吸引大数量应用上架（即使大部分粗制滥造），元服务上架甚至不需要软著和备案；然后再提高门槛，把上车晚的无法引流的垃圾元服务下架，省一点是一点。另一方面，官方的开发者联盟论坛直到今天为止，仍然有大量疑似官方的水军账号大量复制粘贴自导自演的发帖，把正常的虽然比较少的正常开发者的讨论与提问淹没。\n消费者方面： 先是发放各种合作厂商的会员月卡/季卡，再是直接送钱充话费电费交通卡；最后24年快过去了，API 14 先支持 Nova 12/13 用户升级需求，能拉一点升级率算一点。\n可以看出，华为的人力和财力，都花在了在2024年结束前完成 转化率 留存率 活跃度 KPI上；至于生态的质量以及后续发展，真不好说这是否算正面的影响。\n这一切的行为导向，让我在开发这个项目的时候，不断感受到无力与被歧视。首先，API 13/14 的推出，很明显是为了服务头部厂商（比如微信）的反馈（API 13），以及提高用户的升级率的需求（API 14），个人开发者反馈的问题几乎不能得到修复，比如 HUKS 的各种文档缺失以及 ArkTS 的效率低下。\n其次，元服务的生态地位堪忧，虽然官方一直在强调元服务与普通应用一体两面，能给用户带来更独特的体验，但截至目前为止，元服务简介内容不能被应用市场索引，曾一度被禁止上架公开测试（一个很好免费的获流渠道直到被垃圾元服务开发者滥用），不能使用鸿蒙碰一碰分享（ShareKit），不能接入意图框架，卡片加桌API存在缺陷不能触发刷新，商店审核被其他应用插队 …… 等等诸多限制，只能看出官方对元服务生态的漠视。 大概“待到山花烂漫时”，元服务也就算个肥料罢了，真不知道发的 300 块元服务优惠券到底肥了谁的钱包。\n总而言之，经历了这么多波折，我还是完成了这个项目。等这波撒钱/骗钱的热度过去，再看看鸿蒙生态的未来在哪里吧，至少我现在对它是十分失望的（手持 HUAWEI Pura 70 Pro Plus 光织银 16+512GB 原价购入用户 作答 😅）。\n","date":"2024-12-31T19:00:00+08:00","image":"https://sudofree.xyz/p/developer-log-aigis-authenticator-for-harmonyos-next/screenshot-preview_hu16647423754353517212.png","permalink":"https://sudofree.xyz/p/developer-log-aigis-authenticator-for-harmonyos-next/","title":"开发日志：Aigis Authenticator for HarmonyOS NEXT"},{"content":"Retrohos 的目标是作为 OpenHarmony/HarmonyOS 上 RetroArch Frontend 的实现以及完成各社区 Emulator 核心的移植，并同时支持鸿蒙框架关于分布式应用的愿景（如大屏显示同时手机作为手柄的体验）。 由于本人之前只有粗浅的 Android 移动端开发经验（通过NDK控制Android投屏），本日志仅作为应用开发探索过程中的记录，并非最佳实现，仅供参考。\n什么是 Libretro RetroArch 是一个非常流行的怀旧游戏主机解决方案。它本身并不提供任何仿真器的功能 只是大自然的搬运工，只是提供了聚合社区仿真器（Emulator Core）的平台，以及一个通用的接入接口定义 Libretro。以下简介生成自 ChatGPT :\nLibretro 是一个轻量级的游戏机模拟器核心框架，通过 Libretro API，可以将游戏核心（Core）与前端（Frontend）分离，实现核心的跨平台移植，同时支持多种前端，如 RetroArch、Lakka 等。Libretro 核心是一个动态链接库，通过调用 Libretro API，可以实现游戏核心的初始化、输入输出、音频视频渲染等功能。\nLibretro 很好地将模拟器的构成区分开来，使得我们可以忽略现有前端的实现（以及各异的GUI技术栈），专注于核心的移植以及与系统的对接。因此 Retrohos 项目的目标是将实现一个兼容 Libretro API 的前端，同时移植社区的 Emulator Core，而非 RetroArch 的完整移植。\n准备 NDK 环境 目前关于 OpenHarmony 的开发，本人推荐直接使用 HarmonyOS 商业发行版以及华为提供的开发者内测SDK，而非社区开源的 OpenHarmony 版本；虽然两者目前差异度不大（HarmonyOS Next Beta1 基于 OpenHarmony 5.0.0.xx），但目前唯一可用且成熟的生态环境只有 HarmonyOS，这是一个不得不接受的现状。 但好在 OpenHarmony NDK 开发与 Android 类似，并不需要依赖官方提供的IDE（DevEco Studio）；以下的开发内容将以命令行工具为基础，在 Linux 平台（Ubuntu 22.04.3 LTS）上进行。\n以当前最新的官方Linux版本SDK（5.0.0.800）为例，以下假设 commandline-tools.zip 压缩包已经下载并解压到 $HOME 目录下；为了方便区分版本号，将解压后的目录重新命名为 command-line-tools/5.0.0.800，并在其中创建 env.sh 文件，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash export OHOS_ROOT=$HOME/command-line-tools/5.0.3.800 export SDK_ROOT=$OHOS_ROOT/sdk/default/openharmony ## export NDK Path export NDK_ROOT=$SDK_ROOT/native export PATH=$NDK_ROOT/build-tools/cmake/bin:$PATH export PATH=$NDK_ROOT/llvm/bin:$PATH ## export OHOS command line tools export PATH=$OHOS_ROOT/tool/node/bin:$PATH export PATH=$OHOS_ROOT/bin:$PATH 之后在使用时，只需执行 source env.sh 即可。该文件主要起到以下作用：\n设置SDK相关环境变量，这里变量名称没有特殊要求，只是为了方便后续引用； $NDK_ROOT 指向 NDK 的根目录，方便后续引用编译链工具，以及链接 sysroot； $PATH 中前置添加了 cmake 和 llvm 的路径，会覆盖掉系统默认的编译工具链， 以确保使用 NDK 提供的工具链。 另外根据官方NDK开发文档，在调用 NDK 提供的 cmake 编译时，需要传入以下参数（以 aarch64-linux-ohos 为例）：\n1 cmake -DOHOS_STL=c++_shared -DOHOS_ARCH=arm64-v8a -DOHOS_PLATFORM=OHOS -DCMAKE_TOOLCHAIN_FILE=${NDK_ROOT}/build/cmake/ohos.toolchain.cmake . 其中 CMAKE_TOOLCHAIN_FILE 最为重要，指定了编译链的配置文件，该文件会自动链接 NDK 提供的 sysroot。\n移植 Libretro 核心 libretro-super 移植尝试 libretro/libretro-super 仓库是一个用于管理 Libretro 核心以及 RetroArch 在不同平台编译脚本的管理项目，其按照 target triplet 标识与组织不同平台下的构建流程。 最初，我以为 OpenHarmony 底层不过是 Linux kernel 的 aarch64 架构，工具链中的动态库也都很常见，与 SDK关系不大，因此尝试直接移植这个超级项目。然而可惜的是，由于 ohos / OHOS 标识并非众多上游依赖支持的平台，并且它也不会被自动降级到 aarch64 Linux 的构建流程中，暴力移植只会陷入到无限的依赖解决中。\n因此最合适的解决方案，是仅采用 Libretro Core 的构建流程（逐个 Fork 仓库进行修改😓），抛弃 libretro-super 这个过于杂乱的项目的整体移植。在本项目的启动阶段，我们聚焦于 ppsspp 核心的移植：它本身提供了 ppsspp-libretro 的动态库构建选项，三方依赖较少，是第一个成功构建的核心。\n编译 ppsspp-libretro 核心 ppsspp-ffmpeg 静态库构建 hrydgard/ppsspp 通过 .gitmodules 引入了多个三方依赖，其中只有其自己维护的 hrydgard/ppsspp-ffmpeg 需要额外处理 OHOS 架构字符串的添加，参考 android_arm64_v8a.sh 并对应添加 ohos_arm64-v8a.sh 文件内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 #!/bin/bash if [ \u0026#34;$NDK_ROOT\u0026#34; = \u0026#34;\u0026#34; ]; then echo \u0026#34;Please set NDK_ROOT to your OpenHarmony NDK location.\u0026#34; exit 1 fi NDK_PLATFORM=\u0026#34;aarch64-linux-ohos\u0026#34; NDK_PLATFORM_LIB=$NDK_ROOT/sysroot/usr/lib/$NDK_PLATFORM NDK_PREBUILTLLVM=$NDK_ROOT/llvm set -e GENERAL=\u0026#34;\\ --enable-cross-compile \\ --enable-pic \\ --cc=$NDK_PREBUILTLLVM/bin/clang \\ --cross-prefix=$NDK_PREBUILTLLVM/bin/aarch64-unknown-linux-ohos- \\ --ar=$NDK_PREBUILTLLVM/bin/llvm-ar \\ --ld=$NDK_PREBUILTLLVM/bin/clang \\ --nm=$NDK_PREBUILTLLVM/bin/llvm-nm \\ --ranlib=$NDK_PREBUILTLLVM/bin/llvm-ranlib\u0026#34; MODULES=\u0026#34;\\ --disable-avdevice \\ --disable-filters \\ --disable-programs \\ --disable-network \\ --disable-avfilter \\ --disable-postproc \\ --disable-encoders \\ --disable-protocols \\ --disable-hwaccels \\ --disable-doc\u0026#34; VIDEO_DECODERS=\u0026#34;\\ --enable-decoder=h264 \\ --enable-decoder=mpeg4 \\ --enable-decoder=mpeg2video \\ --enable-decoder=mjpeg \\ --enable-decoder=mjpegb\u0026#34; AUDIO_DECODERS=\u0026#34;\\ --enable-decoder=aac \\ --enable-decoder=aac_latm \\ --enable-decoder=atrac3 \\ --enable-decoder=atrac3p \\ --enable-decoder=mp3 \\ --enable-decoder=pcm_s16le \\ --enable-decoder=pcm_s8\u0026#34; DEMUXERS=\u0026#34;\\ --enable-demuxer=h264 \\ --enable-demuxer=m4v \\ --enable-demuxer=mpegvideo \\ --enable-demuxer=mpegps \\ --enable-demuxer=mp3 \\ --enable-demuxer=avi \\ --enable-demuxer=aac \\ --enable-demuxer=pmp \\ --enable-demuxer=oma \\ --enable-demuxer=pcm_s16le \\ --enable-demuxer=pcm_s8 \\ --enable-demuxer=wav\u0026#34; VIDEO_ENCODERS=\u0026#34;\\ --enable-encoder=huffyuv --enable-encoder=ffv1\u0026#34; AUDIO_ENCODERS=\u0026#34;\\ --enable-encoder=pcm_s16le\u0026#34; MUXERS=\u0026#34;\\ --enable-muxer=avi\u0026#34; PARSERS=\u0026#34;\\ --enable-parser=h264 \\ --enable-parser=mpeg4video \\ --enable-parser=mpegaudio \\ --enable-parser=mpegvideo \\ --enable-parser=aac \\ --enable-parser=aac_latm\u0026#34; function build_arm64 { # no-missing-prototypes because of a compile error seemingly unique to aarch64. ./configure --logfile=conflog.txt --target-os=linux \\ --prefix=./ohos/arm64 \\ --arch=aarch64 \\ ${GENERAL} \\ --extra-cflags=\u0026#34; --target=aarch64-linux-ohos -no-canonical-prefixes -fdata-sections -ffunction-sections -fno-limit-debug-info -funwind-tables -fPIC -O2 -DOHOS -DOHOS_ARCH=arm64-v8a -DOHOS_PLATFORM=OHOS -DCMAKE_TOOLCHAIN_FILE=${NDK_ROOT}/build/cmake/ohos.toolchain.cmake -Dipv6mr_interface=ipv6mr_ifindex -fasm -fno-short-enums -fno-strict-aliasing -Wno-missing-prototypes\u0026#34; \\ --disable-shared \\ --enable-static \\ --extra-ldflags=\u0026#34; -B$NDK_PREBUILTLLVM/bin/aarch64-unknown-linux-ohos- --target=aarch64-linux-ohos -Wl,--rpath-link,$NDK_PLATFORM_LIB -L$NDK_PLATFORM_LIB -nostdlib -lc -lm -ldl\u0026#34; \\ --enable-zlib \\ --disable-everything \\ ${MODULES} \\ ${VIDEO_DECODERS} \\ ${AUDIO_DECODERS} \\ ${VIDEO_ENCODERS} \\ ${AUDIO_ENCODERS} \\ ${DEMUXERS} \\ ${MUXERS} \\ ${PARSERS} make clean make -j4 install } build_arm64 通过之前提到的 source env.sh 准备好 NDK 环境后，随后执行该构建脚本，在 ohos/arm64 下生成 libavcodec.a 等静态库文件，以及 include 目录下的头文件。 提交相关更改后，即可在 ppsspp-libretro 中将 submodule 指向我们修改后的仓库 Retrohos/ppsspp-ffmpeg.\nppsspp 构建 ppsspp-libretro 本身的移植较为简单，只需要在 CMakeLists.txt 中添加 OHOS 架构的判断，以及修改 CMakeLists.txt 中的编译选项，patch 内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 diff --git a/CMakeLists.txt b/CMakeLists.txt index ad2be076b..2fe1e619c 100644 --- a/CMakeLists.txt +++ b/CMakeLists.txt @@ -99,8 +99,12 @@ if(${CMAKE_SYSTEM_NAME} MATCHES \u0026#34;Android\u0026#34;) set(ANDROID ON) endif() +if(${CMAKE_SYSTEM_NAME} MATCHES \u0026#34;OHOS\u0026#34;) +\tset(OHOS ON) +endif() + # We only support Vulkan on Unix, macOS (by MoltenVK), Android and Windows. -if(ANDROID OR WIN32 OR (UNIX AND NOT ARM_NO_VULKAN)) +if(OHOS OR ANDROID OR WIN32 OR (UNIX AND NOT ARM_NO_VULKAN)) set(VULKAN ON) endif() @@ -192,7 +196,7 @@ if(USE_CCACHE) include(ccache) endif() -if(UNIX AND NOT (APPLE OR ANDROID) AND VULKAN) +if(UNIX AND NOT (APPLE OR ANDROID OR OHOS) AND VULKAN) if(USING_X11_VULKAN) message(\u0026#34;Using X11 for Vulkan\u0026#34;) find_package(X11) @@ -225,7 +229,7 @@ if(LIBRETRO) endif() endif() -if(ANDROID) +if(ANDROID OR OHOS) set(MOBILE_DEVICE ON) set(USING_GLES2 ON) endif() @@ -382,7 +386,7 @@ if(NOT MSVC) # NEON optimizations in libpng17 seem to cause PNG load errors, see #14485. add_definitions(-DPNG_ARM_NEON_OPT=0) -\tif(ANDROID) +\tif(ANDROID OR OHOS) set(CMAKE_CXX_FLAGS \u0026#34;${CMAKE_CXX_FLAGS} -std=gnu++17\u0026#34;) endif() if(CLANG) @@ -474,7 +478,7 @@ if(NOT MSVC) if(${CMAKE_SYSTEM_NAME} STREQUAL \u0026#34;NetBSD\u0026#34;) add_definitions(-D_NETBSD_SOURCE) endif() -\telseif(ANDROID) +\telseif(ANDROID OR OHOS) add_definitions(-fsigned-char) endif() else() @@ -943,6 +947,10 @@ if(USE_FFMPEG) elseif(X86) set(PLATFORM_ARCH \u0026#34;android/x86\u0026#34;) endif() +\telseif(OHOS) +\tif(ARM64) +\tset(PLATFORM_ARCH \u0026#34;ohos/arm64\u0026#34;) +\tendif() elseif(IOS) if(IOS_PLATFORM STREQUAL \u0026#34;TVOS\u0026#34;) set(PLATFORM_ARCH \u0026#34;tvos/arm64\u0026#34;) @@ -1200,7 +1208,7 @@ else() endif() # Arm platforms require at least libpng17. -if(ANDROID OR ARMV7 OR ARM64 OR ARM OR IOS) +if(OHOS OR ANDROID OR ARMV7 OR ARM64 OR ARM OR IOS) set(PNG_REQUIRED_VERSION 1.7) else() set(PNG_REQUIRED_VERSION 1.6) 修改完成后执行\n1 cmake -DOHOS_STL=c++_shared -DOHOS_ARCH=arm64-v8a -DOHOS_PLATFORM=OHOS -DCMAKE_TOOLCHAIN_FILE=${NDK_ROOT}/build/cmake/ohos.toolchain.cmake -DLIBRETRO=ON -DUSING_EGL=ON -DUSING_GLES2=ON . -Bbuild 创建 build 目录，并在其中执行 make -j4 即可生成 lib/ppsspp_libretro.so 文件。\n后续计划 作为项目的启动阶段，一个核心的移植已经足够我们进行后续的前端开发。接下来计划先设计一个简单的 Libretro 前端，并通过 ArkTS NAPI 进行绑定调用，快速实现 framebuffer 的输出。至于更为重要的 GUI 设计，可以再等等 DevEco Studio 开发套件的完善(～o￣3￣)～\n","date":"2024-09-07T22:14:46+08:00","image":"https://sudofree.xyz/p/retrohos-development-logs-01-migrating-libretro-cores/ppsspp-libretro-core_hu15084682474071154973.png","permalink":"https://sudofree.xyz/p/retrohos-development-logs-01-migrating-libretro-cores/","title":"Retrohos 开发日志其一: 移植 Libretro Cores"},{"content":" 以下内容使用 deepl.com 机翻辅助，如有不适请参考英文版本。\n我很奇怪为什么没有人发现这个内存泄漏问题。也许是我太懒了，没有在网上翻到。 总之，我发现在原始实现中存在明显的内存泄漏问题（其源代码可在 Github 上获取）。\n我在加载一个大型数据集时发现了这个问题，该数据集包含约 600 的图像和 400 万的点云数据，这些图像和点是通过精确的几何映射收集的。训练时占用了NVIDIA RTX 3090 GPU 20GB 以上的显存，同时还占用了 15GB 以上的内存。虽然场景确实很大，但我很好奇为什么一开始就占用了所有内存，而不是在训练过程中逐渐增加！\n第一个补丁 - 延迟加载 首先，我找出了 CPU 占用大量内存（即 RAM 占用）的原因：一开始，所有图像的都在 RAM 中被加载，并在 GPU 上创建了相应的张量。相关源代码见 utils/camera_utils.py#L20：\n1 2 3 4 5 6 7 8 9 def loadCam(args, id, cam_info, resolution_scale, is_test_dataset): ... return Camera(resolution, colmap_id=cam_info.uid, R=cam_info.R, T=cam_info.T, FoVx=cam_info.FovX, FoVy=cam_info.FovY, depth_params=cam_info.depth_params, image=image, invdepthmap=invdepthmap, image_name=cam_info.image_name, uid=id, data_device=args.data_device, train_test_exp=args.train_test_exp, is_test_dataset=is_test_dataset, is_test_view=cam_info.is_test) 我的直接想法是进行权衡：图像加载时间可能不值得消耗这么多内存，尤其是考虑到后续的巨大训练负担。 因此，我编写了一个 LazyLoader 类，将 Camera 类的实例化延迟到第一次引用。实现示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from scene.cameras import Camera class LazyLoader: def __init__(self, cls, *args, **kwargs): self.cls = cls self.args = args self.kwargs = kwargs self.instance = None pass def __getattribute__(self, name: str): if name in [\u0026#39;cls\u0026#39;, \u0026#39;args\u0026#39;, \u0026#39;kwargs\u0026#39;, \u0026#39;instance\u0026#39;]: return super().__getattribute__(name) else: if not self.instance: self.instance = self.cls(*self.args, **self.kwargs) return getattr(self.instance, name) def __del__(self): if self.instance: del self.instance pass pass 因此，Camera 类构建的原始引用修改为\n1 2 - return Camera(colmap_id=cam_info.uid, R=cam_info.R, T=cam_info.T, + return LazyLoader(Camera, colmap_id=cam_info.uid, R=cam_info.R, T=cam_info.T, 在 train.py 中的主训练循环中使用后，图像会被立即删除，直到下一次使用再被创建。\n1 2 3 4 5 6 # Loss gt_image = viewpoint_cam.original_image.cuda() + del viewpoint_cam Ll1 = l1_loss(image, gt_image) loss = (1.0 - opt.lambda_dssim) * Ll1 + opt.lambda_dssim * (1.0 - ssim(image, gt_image)) loss.backward() 然而，奇怪的是，内存使用量在开始时确实是可以接受的，图像会按照预期逐个加载。但最后却增加到了 20GB 以上，之后从未减少过。所以我意识到真正的问题是内存泄漏。\n第二个补丁 - 内存泄漏修复 这里的内存泄漏非常令人困惑。正如我之前提到的，我在 Python 中使用 del 来删除对 Camera 类的引用，所有的内存应该同时被清除。我试图到处寻找原因，但只发现它与 PyTorch 有关。由于无法深入研究 CUDA 张量，我开始尝试在 Python 代码中的 del 所有相关内容。幸运的是，它成功了。\n补丁可分为两部分。第一部分是继续提高图像加载的效率：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 diff --git a/scene/__init__.py b/scene/__init__.py --- a/scene/__init__.py +++ b/scene/__init__.py @@ -37,9 +37,6 @@ class Scene: self.loaded_iter = load_iteration print(\u0026#34;Loading trained model at iteration {}\u0026#34;.format(self.loaded_iter)) - self.train_cameras = {} - self.test_cameras = {} - if os.path.exists(os.path.join(args.source_path, \u0026#34;sparse\u0026#34;)): scene_info = sceneLoadTypeCallbacks[\u0026#34;Colmap\u0026#34;](args.source_path, args.images, args.eval) elif os.path.exists(os.path.join(args.source_path, \u0026#34;transforms_train.json\u0026#34;)): @@ -67,12 +64,8 @@ class Scene: random.shuffle(scene_info.test_cameras) # Multi-res consistent random shuffling self.cameras_extent = scene_info.nerf_normalization[\u0026#34;radius\u0026#34;] - - for resolution_scale in resolution_scales: - print(\u0026#34;Loading Training Cameras\u0026#34;) - self.train_cameras[resolution_scale] = cameraList_from_camInfos(scene_info.train_cameras, resolution_scale, args) - print(\u0026#34;Loading Test Cameras\u0026#34;) - self.test_cameras[resolution_scale] = cameraList_from_camInfos(scene_info.test_cameras, resolution_scale, args) + self.scene_info = scene_info + self.args = args if self.loaded_iter: self.gaussians.load_ply(os.path.join(self.model_path, @@ -87,7 +80,7 @@ class Scene: self.gaussians.save_ply(os.path.join(point_cloud_path, \u0026#34;point_cloud.ply\u0026#34;)) def getTrainCameras(self, scale=1.0): - return self.train_cameras[scale] + return cameraList_from_camInfos(self.scene_info.train_cameras, scale, self.args) def getTestCameras(self, scale=1.0): - return self.test_cameras[scale] \\ No newline at end of file + return cameraList_from_camInfos(self.scene_info.test_cameras, scale, self.args) \\ No newline at end of file diff --git a/scene/cameras.py b/scene/cameras.py --- a/scene/cameras.py +++ b/scene/cameras.py @@ -17,6 +17,7 @@ from utils.graphics_utils import getWorld2View2, getProjectionMatrix class Camera(nn.Module): def __init__(self, colmap_id, R, T, FoVx, FoVy, image, gt_alpha_mask, image_name, uid, + raw_image=None, trans=np.array([0.0, 0.0, 0.0]), scale=1.0, data_device = \u0026#34;cuda\u0026#34; ): super(Camera, self).__init__() @@ -29,6 +30,14 @@ class Camera(nn.Module): self.FoVy = FoVy self.image_name = image_name + if raw_image is not None: + image = raw_image[:3, ...] + gt_alpha_mask = None + + if raw_image.shape[1] == 4: + gt_alpha_mask = raw_image[3:4, ...] + del raw_image + try: self.data_device = torch.device(data_device) except Exception as e: @@ -39,9 +48,11 @@ class Camera(nn.Module): self.original_image = image.clamp(0.0, 1.0).to(self.data_device) self.image_width = self.original_image.shape[2] self.image_height = self.original_image.shape[1] + del image if gt_alpha_mask is not None: self.original_image *= gt_alpha_mask.to(self.data_device) + del gt_alpha_mask else: self.original_image *= torch.ones((1, self.image_height, self.image_width), device=self.data_device) @@ -56,6 +67,9 @@ class Camera(nn.Module): self.full_proj_transform = (self.world_view_transform.unsqueeze(0).bmm(self.projection_matrix.unsqueeze(0))).squeeze(0) self.camera_center = self.world_view_transform.inverse()[3, :3] + def __del__(self): + del self.original_image, self.world_view_transform, self.projection_matrix, self.full_proj_transform, self.camera_center + class MiniCam: def __init__(self, width, height, fovy, fovx, znear, zfar, world_view_transform, full_proj_transform): self.image_width = width diff --git a/train.py b/train.py --- a/train.py +++ b/train.py @@ -44,103 +43,82 @@ def training(dataset, opt, pipe, testing_iterations, saving_iterations, checkpoi iter_start = torch.cuda.Event(enable_timing = True) iter_end = torch.cuda.Event(enable_timing = True) - viewpoint_stack = None + viewpoint_stack = scene.getTrainCameras() ema_loss_for_log = 0.0 progress_bar = tqdm(range(first_iter, opt.iterations), desc=\u0026#34;Training progress\u0026#34;) first_iter += 1 第二部分是在主训练循环中使用 del。因为我对训练过程做了其他修改，因此无法在此给出完整的补丁。缩减版如下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 for iteration in range(first_iter, opt.iterations + 1): iter_start.record() gaussians.update_learning_rate(iteration) # Every 1000 its we increase the levels of SH up to a maximum degree if iteration % 1000 == 0: gaussians.oneupSHdegree() # Pick a random Camera if len(viewpoint_stack)==0: del viewpoint_stack viewpoint_stack = scene.getTrainCameras() viewpoint_cam = viewpoint_stack.pop(randint(0, len(viewpoint_stack)-1)) # Render if (iteration - 1) == debug_from: pipe.debug = True bg = torch.rand((3), device=\u0026#34;cuda\u0026#34;) if opt.random_background else background render_pkg = render(viewpoint_cam, gaussians, pipe, bg) image, viewspace_point_tensor, visibility_filter, radii = render_pkg[\u0026#34;render\u0026#34;], render_pkg[\u0026#34;viewspace_points\u0026#34;], render_pkg[\u0026#34;visibility_filter\u0026#34;], render_pkg[\u0026#34;radii\u0026#34;] # Loss gt_image = viewpoint_cam.original_image.cuda() Ll1 = l1_loss(image, gt_image) loss = (1.0 - opt.lambda_dssim) * Ll1 + opt.lambda_dssim * (1.0 - ssim(image, gt_image)) loss.backward() iter_end.record() with torch.no_grad(): # Progress bar ema_loss_for_log = 0.4 * loss.item() + 0.6 * ema_loss_for_log if iteration % 10 == 0: progress_bar.set_postfix({\u0026#34;Loss\u0026#34;: f\u0026#34;{ema_loss_for_log:.{7}f}\u0026#34;}) progress_bar.update(10) if iteration == opt.iterations: progress_bar.close() # Log and save torch.cuda.empty_cache() if (iteration in saving_iterations): print(\u0026#34;\\n[ITER {}] Saving Gaussians\u0026#34;.format(iteration)) scene.save(iteration) # Densification if iteration \u0026lt; opt.densify_until_iter: # Keep track of max radii in image-space for pruning gaussians.max_radii2D[visibility_filter] = torch.max(gaussians.max_radii2D[visibility_filter], radii[visibility_filter]) gaussians.add_densification_stats(viewspace_point_tensor, visibility_filter) if iteration \u0026gt; opt.densify_from_iter and iteration % opt.densification_interval == 0: size_threshold = 20 if iteration \u0026gt; opt.opacity_reset_interval else None gaussians.densify_and_prune(opt.densify_grad_threshold, 0.005, scene.cameras_extent, size_threshold) if iteration % opt.opacity_reset_interval == 0 or (dataset.white_background and iteration == opt.densify_from_iter): gaussians.reset_opacity() # Optimizer step if iteration \u0026lt; opt.iterations: gaussians.optimizer.step() gaussians.optimizer.zero_grad(set_to_none = True) if (iteration in checkpoint_iterations): print(\u0026#34;\\n[ITER {}] Saving Checkpoint\u0026#34;.format(iteration)) torch.save((gaussians.capture(), iteration), scene.model_path + \u0026#34;/chkpnt\u0026#34; + str(iteration) + \u0026#34;.pth\u0026#34;) del viewpoint_cam, image, viewspace_point_tensor, visibility_filter, radii, gt_image, Ll1, loss, render_pkg 在打了两个补丁之后，内存使用量终于得到了控制。在我们的案例中，GPU 内存使用量可以控制在 10GB 以下。\n结论 我不确定是否应该向原始版本库报告这个问题。因为在过去的一年里，人们并没有意识到内存泄漏问题，所以我不确定这是否是一个普遍问题，或者只是我的情况。总之，我希望这篇文章能帮助正在为同样问题而苦恼的人。\n","date":"2024-09-02T15:43:01+08:00","image":"https://sudofree.xyz/p/solve-memory-leak-in-vanilla-gaussian-splatting-code/memory-leak_hu14931053131067599422.png","permalink":"https://sudofree.xyz/p/solve-memory-leak-in-vanilla-gaussian-splatting-code/","title":"解决 Gaussian Splatting 原始实现中的内存泄漏问题"},{"content":"WIP \u0026hellip;\n","date":"2024-08-30T19:41:50+08:00","permalink":"https://sudofree.xyz/p/distributed-native-application/","title":"Distributed Native Application"}]